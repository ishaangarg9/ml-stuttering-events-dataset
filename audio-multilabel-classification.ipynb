{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audio Multilabel Classification with PyTorch\n",
    "\n",
    "This notebook implements various models for multilabel classification of audio recordings based on extracted features. The dataset contains features from audio recordings with five possible labels: Prolongation, Block, WordRep, SoundRep, and Interjection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load the data\n",
    "def load_data(file_path):\n",
    "    # Read the data\n",
    "    columns = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', \n",
    "               '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', \n",
    "               '30', '31', '32', '33', 'filename', 'Prolongation', 'Block', 'WordRep', 'SoundRep', 'Interjection']\n",
    "    \n",
    "    data = pd.read_csv(file_path, names=columns)\n",
    "    return data\n",
    "\n",
    "# If you have the data loaded directly from the paste.txt file, use this:\n",
    "# Assuming the data is in a file called 'paste.txt'\n",
    "data = load_data('paste.txt')\n",
    "\n",
    "# Display the first few rows to verify\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Separate features and labels\n",
    "# Assuming columns 0-33 are features and the last 5 columns are labels\n",
    "feature_cols = [str(i) for i in range(34)]\n",
    "label_cols = ['Prolongation', 'Block', 'WordRep', 'SoundRep', 'Interjection']\n",
    "\n",
    "X = data[feature_cols].values\n",
    "y = data[label_cols].values\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Check data shapes\n",
    "print(f\"Training features shape: {X_train_scaled.shape}\")\n",
    "print(f\"Testing features shape: {X_test_scaled.shape}\")\n",
    "print(f\"Training labels shape: {y_train.shape}\")\n",
    "print(f\"Testing labels shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Custom PyTorch Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create a custom PyTorch Dataset\n",
    "class AudioFeatureDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = torch.FloatTensor(features)\n",
    "        self.labels = torch.FloatTensor(labels)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.labels[idx]\n",
    "\n",
    "# Create dataset objects\n",
    "train_dataset = AudioFeatureDataset(X_train_scaled, y_train)\n",
    "test_dataset = AudioFeatureDataset(X_test_scaled, y_test)\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 8\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Custom PyTorch Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Simple feed-forward neural network\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size // 2)\n",
    "        self.fc3 = nn.Linear(hidden_size // 2, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "# Deep neural network with dropout for better regularization\n",
    "class DeepNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, dropout_rate=0.3):\n",
    "        super(DeepNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_size)\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size * 2)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_size * 2)\n",
    "        self.dropout2 = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        self.fc3 = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.bn3 = nn.BatchNorm1d(hidden_size)\n",
    "        self.dropout3 = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        self.fc4 = nn.Linear(hidden_size, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        \n",
    "        x = self.fc3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout3(x)\n",
    "        \n",
    "        x = self.fc4(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training and Evaluation Functions for PyTorch Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Training function\n",
    "def train_model(model, train_loader, criterion, optimizer, device, num_epochs=50):\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "        \n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        train_losses.append(epoch_loss)\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0 or epoch == 0:\n",
    "            print(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}')\n",
    "    \n",
    "    return train_losses\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_model(model, test_loader, criterion, device):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    test_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            test_loss += loss.item() * inputs.size(0)\n",
    "            \n",
    "            # Convert outputs to binary predictions (threshold = 0.5)\n",
    "            preds = (outputs > 0.5).float()\n",
    "            \n",
    "            all_preds.append(preds.cpu().numpy())\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "    \n",
    "    # Concatenate batch predictions\n",
    "    all_preds = np.vstack(all_preds)\n",
    "    all_labels = np.vstack(all_labels)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(all_labels.flatten(), all_preds.flatten())\n",
    "    precision = precision_score(all_labels, all_preds, average='samples', zero_division=0)\n",
    "    recall = recall_score(all_labels, all_preds, average='samples', zero_division=0)\n",
    "    f1 = f1_score(all_labels, all_preds, average='samples', zero_division=0)\n",
    "    \n",
    "    # Calculate per-class metrics\n",
    "    per_class_precision = precision_score(all_labels, all_preds, average=None, zero_division=0)\n",
    "    per_class_recall = recall_score(all_labels, all_preds, average=None, zero_division=0)\n",
    "    per_class_f1 = f1_score(all_labels, all_preds, average=None, zero_division=0)\n",
    "    \n",
    "    avg_test_loss = test_loss / len(test_loader.dataset)\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'per_class_precision': per_class_precision,\n",
    "        'per_class_recall': per_class_recall,\n",
    "        'per_class_f1': per_class_f1,\n",
    "        'test_loss': avg_test_loss,\n",
    "        'predictions': all_preds,\n",
    "        'true_labels': all_labels\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training PyTorch Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Set the device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Model parameters\n",
    "input_size = X_train_scaled.shape[1]  # Number of features\n",
    "hidden_size = 64\n",
    "output_size = y_train.shape[1]  # Number of labels\n",
    "learning_rate = 0.001\n",
    "num_epochs = 50\n",
    "\n",
    "# Initialize models\n",
    "simple_nn = SimpleNN(input_size, hidden_size, output_size).to(device)\n",
    "deep_nn = DeepNN(input_size, hidden_size, output_size).to(device)\n",
    "\n",
    "# Loss function and optimizers\n",
    "criterion = nn.BCELoss()\n",
    "simple_optimizer = optim.Adam(simple_nn.parameters(), lr=learning_rate)\n",
    "deep_optimizer = optim.Adam(deep_nn.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the simple neural network\n",
    "print(\"Training Simple Neural Network...\")\n",
    "simple_losses = train_model(simple_nn, train_loader, criterion, simple_optimizer, device, num_epochs)\n",
    "\n",
    "# Train the deep neural network\n",
    "print(\"\\nTraining Deep Neural Network...\")\n",
    "deep_losses = train_model(deep_nn, train_loader, criterion, deep_optimizer, device, num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluate PyTorch Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Evaluate the models\n",
    "print(\"Evaluating Simple Neural Network...\")\n",
    "simple_metrics = evaluate_model(simple_nn, test_loader, criterion, device)\n",
    "\n",
    "print(\"\\nEvaluating Deep Neural Network...\")\n",
    "deep_metrics = evaluate_model(deep_nn, test_loader, criterion, device)\n",
    "\n",
    "# Print metrics for PyTorch models\n",
    "print(\"\\nSimple NN Metrics:\")\n",
    "print(f\"Accuracy: {simple_metrics['accuracy']:.4f}\")\n",
    "print(f\"Precision: {simple_metrics['precision']:.4f}\")\n",
    "print(f\"Recall: {simple_metrics['recall']:.4f}\")\n",
    "print(f\"F1 Score: {simple_metrics['f1']:.4f}\")\n",
    "print(f\"Test Loss: {simple_metrics['test_loss']:.4f}\")\n",
    "\n",
    "print(\"\\nDeep NN Metrics:\")\n",
    "print(f\"Accuracy: {deep_metrics['accuracy']:.4f}\")\n",
    "print(f\"Precision: {deep_metrics['precision']:.4f}\")\n",
    "print(f\"Recall: {deep_metrics['recall']:.4f}\")\n",
    "print(f\"F1 Score: {deep_metrics['f1']:.4f}\")\n",
    "print(f\"Test Loss: {deep_metrics['test_loss']:.4f}\")\n",
    "\n",
    "# Compare per-class metrics for the Deep NN\n",
    "label_names = ['Prolongation', 'Block', 'WordRep', 'SoundRep', 'Interjection']\n",
    "print(\"\\nPer-Class Metrics for Deep NN:\")\n",
    "for i, label in enumerate(label_names):\n",
    "    print(f\"{label}:\")\n",
    "    print(f\"  Precision: {deep_metrics['per_class_precision'][i]:.4f}\")\n",
    "    print(f\"  Recall: {deep_metrics['per_class_recall'][i]:.4f}\")\n",
    "    print(f\"  F1 Score: {deep_metrics['per_class_f1'][i]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Implement and Evaluate Scikit-learn Models for Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Function to train and evaluate a scikit-learn model\n",
    "def train_evaluate_sklearn_model(model, X_train, y_train, X_test, y_test):\n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test.flatten(), y_pred.flatten())\n",
    "    precision = precision_score(y_test, y_pred, average='samples', zero_division=0)\n",
    "    recall = recall_score(y_test, y_pred, average='samples', zero_division=0)\n",
    "    f1 = f1_score(y_test, y_pred, average='samples', zero_division=0)\n",
    "    \n",
    "    # Calculate per-class metrics\n",
    "    per_class_precision = precision_score(y_test, y_pred, average=None, zero_division=0)\n",
    "    per_class_recall = recall_score(y_test, y_pred, average=None, zero_division=0)\n",
    "    per_class_f1 = f1_score(y_test, y_pred, average=None, zero_division=0)\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'per_class_precision': per_class_precision,\n",
    "        'per_class_recall': per_class_recall,\n",
    "        'per_class_f1': per_class_f1,\n",
    "        'predictions': y_pred,\n",
    "        'true_labels': y_test\n",
    "    }\n",
    "\n",
    "# Initialize scikit-learn models\n",
    "# 1. Random Forest\n",
    "print(\"Training Random Forest...\")\n",
    "rf_model = MultiOutputClassifier(RandomForestClassifier(n_estimators=100, random_state=42))\n",
    "rf_metrics = train_evaluate_sklearn_model(rf_model, X_train_scaled, y_train, X_test_scaled, y_test)\n",
    "\n",
    "# 2. Gradient Boosting\n",
    "print(\"\\nTraining Gradient Boosting...\")\n",
    "gb_model = MultiOutputClassifier(GradientBoostingClassifier(n_estimators=100, random_state=42))\n",
    "gb_metrics = train_evaluate_sklearn_model(gb_model, X_train_scaled, y_train, X_test_scaled, y_test)\n",
    "\n",
    "# 3. Support Vector Machine\n",
    "print(\"\\nTraining SVM...\")\n",
    "svm_model = MultiOutputClassifier(SVC(probability=True, random_state=42))\n",
    "svm_metrics = train_evaluate_sklearn_model(svm_model, X_train_scaled, y_train, X_test_scaled, y_test)\n",
    "\n",
    "# 4. Multi-layer Perceptron (MLP)\n",
    "print(\"\\nTraining MLP...\")\n",
    "mlp_model = MultiOutputClassifier(MLPClassifier(hidden_layer_sizes=(64, 32), max_iter=300, random_state=42))\n",
    "mlp_metrics = train_evaluate_sklearn_model(mlp_model, X_train_scaled, y_train, X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Print metrics for scikit-learn models\n",
    "print(\"\\nRandom Forest Metrics:\")\n",
    "print(f\"Accuracy: {rf_metrics['accuracy']:.4f}\")\n",
    "print(f\"Precision: {rf_metrics['precision']:.4f}\")\n",
    "print(f\"Recall: {rf_metrics['recall']:.4f}\")\n",
    "print(f\"F1 Score: {rf_metrics['f1']:.4f}\")\n",
    "\n",
    "print(\"\\nGradient Boosting Metrics:\")\n",
    "print(f\"Accuracy: {gb_metrics['accuracy']:.4f}\")\n",
    "print(f\"Precision: {gb_metrics['precision']:.4f}\")\n",
    "print(f\"Recall: {gb_metrics['recall']:.4f}\")\n",
    "print(f\"F1 Score: {gb_metrics['f1']:.4f}\")\n",
    "\n",
    "print(\"\\nSVM Metrics:\")\n",
    "print(f\"Accuracy: {svm_metrics['accuracy']:.4f}\")\n",
    "print(f\"Precision: {svm_metrics['precision']:.4f}\")\n",
    "print(f\"Recall: {svm_metrics['recall']:.4f}\")\n",
    "print(f\"F1 Score: {svm_metrics['f1']:.4f}\")\n",
    "\n",
    "print(\"\\nMLP Metrics:\")\n",
    "print(f\"Accuracy: {mlp_metrics['accuracy']:.4f}\")\n",
    "print(f\"Precision: {mlp_metrics['precision']:.4f}\")\n",
    "print(f\"Recall: {mlp_metrics['recall']:.4f}\")\n",
    "print(f\"F1 Score: {mlp_metrics['f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Compare All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Collect all model metrics for comparison\n",
    "model_names = ['Simple NN', 'Deep NN', 'Random Forest', 'Gradient Boosting', 'SVM', 'MLP']\n",
    "metrics = [simple_metrics, deep_metrics, rf_metrics, gb_metrics, svm_metrics, mlp_metrics]\n",
    "\n",
    "# Create comparison dataframe\n",
    "comparison_data = {\n",
    "    'Model': model_names,\n",
    "    'Accuracy': [m['accuracy'] for m in metrics],\n",
    "    'Precision': [m['precision'] for m in metrics],\n",
    "    'Recall': [m['recall'] for m in metrics],\n",
    "    'F1 Score': [m['f1'] for m in metrics]\n",
    "}\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "print(\"Model Comparison:\")\n",
    "comparison_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Visualize the comparison\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Create a bar chart for each metric\n",
    "metrics_to_plot = ['Accuracy', 'Precision', 'Recall', 'F1 Score']\n",
    "x = np.arange(len(model_names))\n",
    "width = 0.2\n",
    "offsets = np.arange(-1.5, 2.5) * width\n",
    "\n",
    "for i, metric in enumerate(metrics_to_plot):\n",
    "    plt.bar(x + offsets[i], comparison_df[metric], width=width, label=metric)\n",
    "\n",
    "plt.xlabel('Models')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Comparison of Model Performance')\n",
    "plt.xticks(x, model_names, rotation=45)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Per-class Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Analyze per-class performance for the best model\n",
    "# Assuming Deep NN is one of the best models (you can change this based on results)\n",
    "best_model_metrics = deep_metrics\n",
    "best_model_name = \"Deep NN\"\n",
    "\n",
    "# Create per-class comparison dataframe\n",
    "per_class_data = {\n",
    "    'Label': label_names,\n",
    "    'Precision': best_model_metrics['per_class_precision'],\n",
    "    'Recall': best_model_metrics['per_class_recall'],\n",
    "    'F1 Score': best_model_metrics['per_class_f1']\n",
    "}\n",
    "\n",
    "per_class_df = pd.DataFrame(per_class_data)\n",
    "print(f\"Per-class Performance for {best_model_name}:\")\n",
    "per_class_df"
   ]
  }
]
}
  